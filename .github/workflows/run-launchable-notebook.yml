name: Run Launchable Notebook

on:
  # Manual trigger
  workflow_dispatch:
  # Trigger on push to main/master/github-action-workflow
  push:
    branches:
      - main
      - master
      - github-action-workflow
  # Trigger on PR to main/master/github-action-workflow
  pull_request:
    branches:
      - main
      - master
      - github-action-workflow

jobs:
  run-notebook:
    runs-on: arc-runner-set-oke-org-poc-4-gpu
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Download notebook_runner_nbclient.py
        run: |
          # Clone blueprint-github-test private repository and extract notebook runner
          # Using GitHub Personal Access Token for authentication
          # Token is stored in GitHub Secrets: BLUEPRINT_GITHUB_TEST_TOKEN
          
          # Clone the repository using token authentication
          git clone https://oauth2:${{ secrets.BLUEPRINT_GITHUB_TEST_TOKEN }}@github.com/NVIDIA-AI-Blueprints/blueprint-github-test.git blueprint-github-test-temp
          
          # Copy the notebook runner file
          cp blueprint-github-test-temp/utils/notebook_runner/notebook_runner_nbclient.py ./notebook_runner_nbclient.py
          
          # Cleanup temporary directory
          rm -rf blueprint-github-test-temp
          
          chmod +x notebook_runner_nbclient.py
          echo "Downloaded notebook_runner_nbclient.py from NVIDIA-AI-Blueprints/blueprint-github-test"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install nbclient and jupyter for notebook execution
          # nbclient provides fine-grained control over cell execution
          pip install nbclient jupyter
          # Install requests for RAG server health check waiting logic in workflow
          pip install requests
          # Pre-install notebook dependencies to avoid kernel restart issue
          # Cell 1 installs these packages, but Cell 3 tries to restart kernel
          # which causes kernel death in automated execution environment
          pip install python-dotenv aiohttp

      - name: Install and register Jupyter kernel
        run: |
          # Install ipykernel for Python kernel support
          pip install ipykernel
          
          # Register Python kernel for Jupyter
          python3 -m ipykernel install --user --name python3 --display-name "Python 3"

      - name: Set up environment variables
        run: |
          # Set MODEL_DIRECTORY for model caching
          echo "MODEL_DIRECTORY=$HOME/.cache/model-cache" >> $GITHUB_ENV
          mkdir -p $HOME/.cache/model-cache
          
          # Set USERID which is required by Docker containers
          echo "USERID=$(id -u)" >> $GITHUB_ENV
          
          # Ensure Jupyter runtime directory exists and set permissions
          python3 << 'EOF'
          from jupyter_core.paths import jupyter_runtime_dir
          import os
          runtime_dir = jupyter_runtime_dir()
          os.makedirs(runtime_dir, exist_ok=True)
          os.chmod(runtime_dir, 0o755)
          EOF
          
          # Set Jupyter environment variables for kernel execution
          echo "JUPYTER_RUNTIME_DIR=$(python3 -c 'from jupyter_core.paths import jupyter_runtime_dir; print(jupyter_runtime_dir())')" >> $GITHUB_ENV
          echo "JUPYTER_DATA_DIR=$(python3 -c 'from jupyter_core.paths import jupyter_data_dir; print(jupyter_data_dir())')" >> $GITHUB_ENV

      - name: Check NGC_API_KEY secret
        run: |
          if [ -z "${{ secrets.NGC_API_KEY }}" ]; then
            echo "Warning: NGC_API_KEY secret is not set in GitHub repository settings"
            echo "The notebook execution may fail if NGC_API_KEY is required"
            echo "Please set NGC_API_KEY in: Settings -> Secrets and variables -> Actions"
          else
            echo "NGC_API_KEY secret is configured"
          fi

      - name: Create temporary notebook copy and add skip tag
        run: |
          # Get git commit SHA (first 7 characters) for unique temporary file naming
          COMMIT_SHA=$(git rev-parse --short=7 HEAD)
          export COMMIT_SHA  # Export to make it available to Python subprocess
          echo "COMMIT_SHA=$COMMIT_SHA" >> $GITHUB_ENV  # Save to GitHub Actions env for later steps
          echo "Using commit SHA for temporary notebook: $COMMIT_SHA"
          
          # Create temporary copy of notebook to avoid modifying original
          # Use commit SHA instead of 'temp' for better traceability
          cp notebooks/launchable.ipynb notebooks/launchable.$COMMIT_SHA.ipynb
          
          # Add ci-skip tag to Cell 3 (kernel restart cell) in temporary copy
          # This allows nbclient to skip execution while preserving original code in HTML output
          python << 'EOF'
          import json
          import os
          
          commit_sha = os.environ['COMMIT_SHA']  # Access exported bash variable
          notebook_path = f"notebooks/launchable.{commit_sha}.ipynb"
          
          with open(notebook_path, 'r', encoding='utf-8') as f:
              nb = json.load(f)
          
          # Find the cell that contains kernel shutdown code (Cell 3)
          found = False
          for i, cell in enumerate(nb['cells']):
              if cell['cell_type'] == 'code':
                  source = ''.join(cell.get('source', []))
                  if 'do_shutdown' in source or 'kernel.do_shutdown' in source:
                      # Add ci-skip tag to metadata
                      if 'metadata' not in cell:
                          cell['metadata'] = {}
                      if 'tags' not in cell['metadata']:
                          cell['metadata']['tags'] = []
                      if 'ci-skip' not in cell['metadata']['tags']:
                          cell['metadata']['tags'].append('ci-skip')
                      print(f"SUCCESS: Added ci-skip tag to cell index {i} (display as Cell {i+1})")
                      print(f"Cell content preview: {source[:100]}")
                      found = True
                      break
          
          if not found:
              print("WARNING: Kernel restart cell not found! Cell 3 will be executed.")
              print("Searching for cells with 'do_shutdown' keyword...")
              for i, cell in enumerate(nb['cells']):
                  if cell['cell_type'] == 'code':
                      source = ''.join(cell.get('source', []))
                      if 'shutdown' in source.lower() or 'restart' in source.lower():
                          print(f"  Cell {i}: {source[:80]}")
          
          with open(notebook_path, 'w', encoding='utf-8') as f:
              json.dump(nb, f, indent=1, ensure_ascii=False)
          
          # Verify tags were saved correctly
          print("\nVerifying tags in saved notebook...")
          with open(notebook_path, 'r', encoding='utf-8') as f:
              nb_check = json.load(f)
          
          for i, cell in enumerate(nb_check['cells']):
              if cell['cell_type'] == 'code':
                  tags = cell.get('metadata', {}).get('tags', [])
                  if tags:
                      print(f"  Cell {i} has tags: {tags}")
          EOF

      - name: Run notebook with notebook_runner_nbclient.py
        id: run_notebook
        run: |
          # Execute notebook runner with output monitoring and subprocess pause/resume mechanism
          # When health check cell is about to execute, pause the subprocess using SIGSTOP
          # Background thread waits for HTTP endpoint to be ready, then resumes subprocess using SIGCONT
          # This prevents ServerDisconnectedError while avoiding deadlock from blocking output monitor
          python3 << 'EOF'
          import subprocess
          import sys
          import time
          import requests
          import threading
          import queue
          import os
          import signal
          
          # Output monitoring queue
          output_queue = queue.Queue()
          
          # Global process reference for pause/resume
          notebook_process = None
          
          # Flag to track if process was paused
          process_paused = threading.Event()
          
          def wait_for_rag_server_and_resume():
              """Background thread to wait for RAG server HTTP health endpoint to be ready.
              
              Once ready, resume the paused notebook subprocess using SIGCONT.
              This runs in a separate thread to avoid blocking the output monitor.
              Only checks HTTP endpoint, ignoring container status (container "Up" != HTTP API ready).
              """
              base_url = "http://0.0.0.0:8081"
              health_url = f"{base_url}/v1/health"
              max_wait = 60  # 60 seconds - reasonable timeout for service startup
              check_interval = 2  # 2 seconds between checks
              start_time = time.time()
              attempt = 0
              
              print("[WORKFLOW] Background thread: Waiting for HTTP health endpoint to be ready...", flush=True)
              
              while time.time() - start_time < max_wait:
                  attempt += 1
                  try:
                      response = requests.get(health_url, timeout=5)
                      if response.status_code == 200:
                          elapsed = time.time() - start_time
                          print(f"[WORKFLOW] SUCCESS: RAG server HTTP endpoint is ready! (waited {elapsed:.1f}s, {attempt} attempts)", flush=True)
                          
                          # Resume the paused process if it was paused
                          if process_paused.is_set() and notebook_process:
                              print("[WORKFLOW] Resuming notebook execution with SIGCONT...", flush=True)
                              notebook_process.send_signal(signal.SIGCONT)
                          return
                  except requests.exceptions.RequestException:
                      elapsed = time.time() - start_time
                      # Log periodically to show progress
                      if attempt % 5 == 0:  # Every 10 seconds (5 attempts * 2 seconds)
                          print(f"[WORKFLOW] Still waiting for HTTP endpoint... ({elapsed:.0f}s elapsed, attempt {attempt})", flush=True)
                  
                  time.sleep(check_interval)
              
              elapsed = time.time() - start_time
              print(f"[WORKFLOW] WARNING: RAG server did not respond within {max_wait}s (waited {elapsed:.1f}s, {attempt} attempts)", flush=True)
              
              # Resume process anyway to let it fail gracefully rather than hanging indefinitely
              if process_paused.is_set() and notebook_process:
                  print("[WORKFLOW] Resuming notebook execution despite timeout (may encounter errors)...", flush=True)
                  notebook_process.send_signal(signal.SIGCONT)
          
          def monitor_output(pipe, queue):
              """Monitor output and detect RAG server startup.
              
              When RAG server startup is detected:
              1. Immediately pause the subprocess using SIGSTOP
              2. Launch a background thread to poll HTTP health API
              3. Once API returns 200, background thread resumes subprocess with SIGCONT
              
              This approach ensures health API is ready before notebook continues execution,
              preventing ServerDisconnectedError while avoiding output monitor blocking (no deadlock).
              """
              paused_for_rag = False  # Flag to ensure we only pause once for RAG startup
              for line in iter(pipe.readline, ''):
                  if line:
                      queue.put(line)
                      sys.stdout.write(line)
                      sys.stdout.flush()
                      
                      # Detect RAG server startup - immediately pause and wait for health API
                      if not paused_for_rag and ('Starting RAG microservices' in line or 'Starting RAG microservices...' in line):
                          print("\n[WORKFLOW] Detected RAG server startup signal", flush=True)
                          
                          # Immediately pause the subprocess to prevent health checks from running too early
                          if notebook_process:
                              print("[WORKFLOW] Pausing notebook execution with SIGSTOP...", flush=True)
                              notebook_process.send_signal(signal.SIGSTOP)
                              process_paused.set()
                              paused_for_rag = True
                          
                          # Launch background thread to poll health API and resume when ready
                          # This does NOT block the output monitor
                          wait_thread = threading.Thread(target=wait_for_rag_server_and_resume, daemon=True)
                          wait_thread.start()
              pipe.close()
          
          # Start notebook runner process
          env = os.environ.copy()
          commit_sha = env.get('COMMIT_SHA', 'unknown')
          cmd = [
              'python', 'notebook_runner_nbclient.py',
              '-f', f'notebooks/launchable.{commit_sha}.ipynb',
              '--output-dir', 'notebook_output',
              '--skip-tags', 'ci-skip',
              '-e', f'NGC_API_KEY={env.get("NGC_API_KEY", "")}'
          ]
          
          process = subprocess.Popen(
              cmd,
              stdout=subprocess.PIPE,
              stderr=subprocess.STDOUT,
              text=True,
              bufsize=1,
              env=env
          )
          
          # Save process reference globally for pause/resume mechanism
          notebook_process = process
          
          # Monitor output in separate thread
          # The monitor will pause subprocess when health check is detected
          # Background thread will resume it once HTTP endpoint is ready
          monitor_thread = threading.Thread(target=monitor_output, args=(process.stdout, output_queue), daemon=True)
          monitor_thread.start()
          
          # Wait for process to complete
          return_code = process.wait()
          
          # Wait for monitoring thread to finish
          monitor_thread.join(timeout=1)
          
          sys.exit(return_code)
          EOF
        env:
          # NGC_API_KEY is required for:
          # 1. Authenticating with NGC container registry (nvcr.io) to pull NIM images
          # 2. Accessing NVIDIA hosted NIM endpoints for LLM inference
          # 3. Downloading models from NGC catalog
          # This must be set as a GitHub secret in repository settings:
          # Settings -> Secrets and variables -> Actions -> New repository secret
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
          
          # MODEL_DIRECTORY is set in previous step for model caching
          MODEL_DIRECTORY: ${{ env.MODEL_DIRECTORY }}
          
          # USERID is set in previous step for Docker container user permissions
          USERID: ${{ env.USERID }}

      - name: Cleanup temporary notebook copy
        if: always()
        run: |
          # Remove temporary notebook copy after execution
          # Use COMMIT_SHA from environment variable
          rm -f notebooks/launchable.${{ env.COMMIT_SHA }}.ipynb

      - name: Wait for RAG Playground to be ready
        run: |
          # Wait for rag-frontend to be ready before running tests
          # RAG backend health check in notebook execution only confirms backend is ready
          # Frontend (rag-frontend:3000) may still be starting up
          # Note: Service was renamed from rag-playground to rag-frontend
          python3 << 'EOF'
          import time
          import subprocess
          import sys
          
          playground_url = "http://rag-frontend:3000"
          max_wait = 60  # 60 seconds timeout
          check_interval = 2  # Check every 2 seconds
          start_time = time.time()
          attempt = 0
          
          print(f"Waiting for RAG Playground to be ready at {playground_url}...")
          
          while time.time() - start_time < max_wait:
              attempt += 1
              try:
                  # Use curl from within docker container on nvidia-rag network
                  # This ensures we're checking from the same network context as tests
                  result = subprocess.run(
                      [
                          'docker', 'run', '--rm', '--network', 'nvidia-rag',
                          'curlimages/curl:latest',
                          'curl', '-s', '-o', '/dev/null', '-w', '%{http_code}',
                          playground_url
                      ],
                      capture_output=True,
                      text=True,
                      timeout=5
                  )
                  
                  if result.returncode == 0:
                      http_code = result.stdout.strip()
                      if http_code.startswith('2') or http_code.startswith('3'):  # 2xx or 3xx
                          elapsed = time.time() - start_time
                          print(f"SUCCESS: RAG Playground is ready (HTTP {http_code}, waited {elapsed:.1f}s, {attempt} attempts)")
                          sys.exit(0)
              except Exception as e:
                  pass
              
              elapsed = time.time() - start_time
              if attempt % 5 == 0:  # Log every 10 seconds
                  print(f"Still waiting for RAG Playground... ({elapsed:.0f}s elapsed, attempt {attempt})")
              
              time.sleep(check_interval)
          
          elapsed = time.time() - start_time
          print(f"WARNING: RAG Playground did not respond within {max_wait}s (waited {elapsed:.1f}s, {attempt} attempts)")
          print("Tests may fail if playground is not ready")
          sys.exit(1)
          EOF

      - name: Test RAG with generated launchable HTML - notebook
        if: always()
        run: |
          COMMIT_SHA="${{ env.COMMIT_SHA }}"
          docker run --network nvidia-rag \
            -v ./notebook_output/launchable.$COMMIT_SHA.html:/app/input/rag_2_3_launchable.html \
            -v "$(pwd):/workspace" \
            nvcr.io/rw983xdqtcdp/auto_test_team/blueprint-github-test-image:latest \
            pytest -m "rag and notebook" --rag-playground-url="http://rag-frontend:3000" --api-url="http://rag-server:8081" --disable-warnings --self-contained-html --html=/workspace/rag_${COMMIT_SHA}_test_report_notebook.html

      - name: Test RAG with generated launchable HTML - UI
        if: always()
        run: |
          COMMIT_SHA="${{ env.COMMIT_SHA }}"
          docker run --network nvidia-rag \
            -v ./notebook_output/launchable.$COMMIT_SHA.html:/app/input/rag_2_3_launchable.html \
            -v "$(pwd):/workspace" \
            nvcr.io/rw983xdqtcdp/auto_test_team/blueprint-github-test-image:latest \
            pytest -m "rag and ui" --rag-playground-url="http://rag-frontend:3000" --api-url="http://rag-server:8081" --disable-warnings --self-contained-html --html=/workspace/rag_${COMMIT_SHA}_test_report_ui.html

      - name: Upload generated notebook test report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rag-generated-test-report
          path: rag_${{ env.COMMIT_SHA }}_test_report_notebook.html
          retention-days: 30

      - name: Upload generated notebook test report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rag-generated-test-report
          path: rag_${{ env.COMMIT_SHA }}_test_report_ui.html
          retention-days: 30

      - name: Upload HTML output
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: launchable-notebook-html
          path: notebook_output/*.html
          retention-days: 30

      - name: Upload executed notebook
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: launchable-notebook-executed
          path: notebook_output/*.executed.ipynb
          retention-days: 30

      - name: Upload complete output directory
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: notebook-runner-complete-output
          path: notebook_output/
          retention-days: 30
