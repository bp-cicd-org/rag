name: Run Launchable Notebook

on:
  # Manual trigger
  workflow_dispatch:
  # Trigger on push to main/master/github-action-workflow when notebook or workflow changes
  push:
    branches:
      - main
      - master
      - github-action-workflow
    paths:
      - 'notebooks/launchable.ipynb'
      - '.github/workflows/run-launchable-notebook.yml'
  # Trigger on PR when notebook or workflow changes
  pull_request:
    paths:
      - 'notebooks/launchable.ipynb'
      - '.github/workflows/run-launchable-notebook.yml'

jobs:
  run-notebook:
    runs-on: arc-runner-set-oke-org-poc-4-gpu
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Download notebook_runner_nbclient.py
        run: |
          # Download notebook_runner_nbclient.py from Blueprint-Utils repository
          # This is a generic notebook runner that works with any Jupyter notebook
          curl -o notebook_runner_nbclient.py \
            https://raw.githubusercontent.com/nv-blueprint-org/Blueprint-Utils/refs/heads/main/notebook_runner/notebook_runner_nbclient.py
          chmod +x notebook_runner_nbclient.py
          echo "Downloaded notebook_runner_nbclient.py from Blueprint-Utils"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install nbclient and jupyter for notebook execution
          # nbclient provides fine-grained control over cell execution
          pip install nbclient jupyter
          # Install requests for RAG server health check waiting logic in workflow
          pip install requests
          # Pre-install notebook dependencies to avoid kernel restart issue
          # Cell 1 installs these packages, but Cell 3 tries to restart kernel
          # which causes kernel death in automated execution environment
          pip install python-dotenv aiohttp

      - name: Install and register Jupyter kernel
        run: |
          # Install ipykernel for Python kernel support
          pip install ipykernel
          
          # Register Python kernel for Jupyter
          python3 -m ipykernel install --user --name python3 --display-name "Python 3"

      - name: Set up environment variables
        run: |
          # Set MODEL_DIRECTORY for model caching
          echo "MODEL_DIRECTORY=$HOME/.cache/model-cache" >> $GITHUB_ENV
          mkdir -p $HOME/.cache/model-cache
          
          # Set USERID which is required by Docker containers
          echo "USERID=$(id -u)" >> $GITHUB_ENV
          
          # Ensure Jupyter runtime directory exists and set permissions
          python3 << 'EOF'
          from jupyter_core.paths import jupyter_runtime_dir
          import os
          runtime_dir = jupyter_runtime_dir()
          os.makedirs(runtime_dir, exist_ok=True)
          os.chmod(runtime_dir, 0o755)
          EOF
          
          # Set Jupyter environment variables for kernel execution
          echo "JUPYTER_RUNTIME_DIR=$(python3 -c 'from jupyter_core.paths import jupyter_runtime_dir; print(jupyter_runtime_dir())')" >> $GITHUB_ENV
          echo "JUPYTER_DATA_DIR=$(python3 -c 'from jupyter_core.paths import jupyter_data_dir; print(jupyter_data_dir())')" >> $GITHUB_ENV

      - name: Check NGC_API_KEY secret
        run: |
          if [ -z "${{ secrets.NGC_API_KEY }}" ]; then
            echo "Warning: NGC_API_KEY secret is not set in GitHub repository settings"
            echo "The notebook execution may fail if NGC_API_KEY is required"
            echo "Please set NGC_API_KEY in: Settings -> Secrets and variables -> Actions"
          else
            echo "NGC_API_KEY secret is configured"
          fi

      - name: Create temporary notebook copy and add skip tag
        run: |
          # Create temporary copy of notebook to avoid modifying original
          cp notebooks/launchable.ipynb notebooks/launchable.temp.ipynb
          
          # Add ci-skip tag to Cell 3 (kernel restart cell) in temporary copy
          # This allows nbclient to skip execution while preserving original code in HTML output
          python << 'EOF'
          import json
          
          notebook_path = "notebooks/launchable.temp.ipynb"
          
          with open(notebook_path, 'r', encoding='utf-8') as f:
              nb = json.load(f)
          
          # Find the cell that contains kernel shutdown code (Cell 3)
          for i, cell in enumerate(nb['cells']):
              if cell['cell_type'] == 'code':
                  source = ''.join(cell.get('source', []))
                  if 'do_shutdown' in source or 'kernel.do_shutdown' in source:
                      # Add ci-skip tag to metadata
                      if 'metadata' not in cell:
                          cell['metadata'] = {}
                      if 'tags' not in cell['metadata']:
                          cell['metadata']['tags'] = []
                      if 'ci-skip' not in cell['metadata']['tags']:
                          cell['metadata']['tags'].append('ci-skip')
                      print(f"Added ci-skip tag to Cell {i+1} (kernel restart cell)")
                      break
          
          with open(notebook_path, 'w', encoding='utf-8') as f:
              json.dump(nb, f, indent=1, ensure_ascii=False)
          EOF

      - name: Run notebook with notebook_runner_nbclient.py
        id: run_notebook
        run: |
          # Execute notebook runner with output monitoring and subprocess pause/resume mechanism
          # When health check cell is about to execute, pause the subprocess using SIGSTOP
          # Background thread waits for HTTP endpoint to be ready, then resumes subprocess using SIGCONT
          # This prevents ServerDisconnectedError while avoiding deadlock from blocking output monitor
          python3 << 'EOF'
          import subprocess
          import sys
          import time
          import requests
          import threading
          import queue
          import os
          import signal
          
          # Output monitoring queue
          output_queue = queue.Queue()
          
          # Global process reference for pause/resume
          notebook_process = None
          
          # Flag to track if process was paused
          process_paused = threading.Event()
          
          def wait_for_rag_server_and_resume():
              """Background thread to wait for RAG server HTTP health endpoint to be ready.
              
              Once ready, resume the paused notebook subprocess using SIGCONT.
              This runs in a separate thread to avoid blocking the output monitor.
              Only checks HTTP endpoint, ignoring container status (container "Up" != HTTP API ready).
              """
              base_url = "http://0.0.0.0:8081"
              health_url = f"{base_url}/v1/health"
              max_wait = 60  # 60 seconds - reasonable timeout for service startup
              check_interval = 2  # 2 seconds between checks
              start_time = time.time()
              attempt = 0
              
              print("[WORKFLOW] Background thread: Waiting for HTTP health endpoint to be ready...", flush=True)
              
              while time.time() - start_time < max_wait:
                  attempt += 1
                  try:
                      response = requests.get(health_url, timeout=5)
                      if response.status_code == 200:
                          elapsed = time.time() - start_time
                          print(f"[WORKFLOW] SUCCESS: RAG server HTTP endpoint is ready! (waited {elapsed:.1f}s, {attempt} attempts)", flush=True)
                          
                          # Resume the paused process if it was paused
                          if process_paused.is_set() and notebook_process:
                              print("[WORKFLOW] Resuming notebook execution with SIGCONT...", flush=True)
                              notebook_process.send_signal(signal.SIGCONT)
                          return
                  except requests.exceptions.RequestException:
                      elapsed = time.time() - start_time
                      # Log periodically to show progress
                      if attempt % 5 == 0:  # Every 10 seconds (5 attempts * 2 seconds)
                          print(f"[WORKFLOW] Still waiting for HTTP endpoint... ({elapsed:.0f}s elapsed, attempt {attempt})", flush=True)
                  
                  time.sleep(check_interval)
              
              elapsed = time.time() - start_time
              print(f"[WORKFLOW] WARNING: RAG server did not respond within {max_wait}s (waited {elapsed:.1f}s, {attempt} attempts)", flush=True)
              
              # Resume process anyway to let it fail gracefully rather than hanging indefinitely
              if process_paused.is_set() and notebook_process:
                  print("[WORKFLOW] Resuming notebook execution despite timeout (may encounter errors)...", flush=True)
                  notebook_process.send_signal(signal.SIGCONT)
          
          def monitor_output(pipe, queue):
              """Monitor output and detect RAG server startup.
              
              When RAG server startup is detected:
              1. Immediately pause the subprocess using SIGSTOP
              2. Launch a background thread to poll HTTP health API
              3. Once API returns 200, background thread resumes subprocess with SIGCONT
              
              This approach ensures health API is ready before notebook continues execution,
              preventing ServerDisconnectedError while avoiding output monitor blocking (no deadlock).
              """
              paused_for_rag = False  # Flag to ensure we only pause once for RAG startup
              for line in iter(pipe.readline, ''):
                  if line:
                      queue.put(line)
                      sys.stdout.write(line)
                      sys.stdout.flush()
                      
                      # Detect RAG server startup - immediately pause and wait for health API
                      if not paused_for_rag and ('Starting RAG microservices' in line or 'Starting RAG microservices...' in line):
                          print("\n[WORKFLOW] Detected RAG server startup signal", flush=True)
                          
                          # Immediately pause the subprocess to prevent health checks from running too early
                          if notebook_process:
                              print("[WORKFLOW] Pausing notebook execution with SIGSTOP...", flush=True)
                              notebook_process.send_signal(signal.SIGSTOP)
                              process_paused.set()
                              paused_for_rag = True
                          
                          # Launch background thread to poll health API and resume when ready
                          # This does NOT block the output monitor
                          wait_thread = threading.Thread(target=wait_for_rag_server_and_resume, daemon=True)
                          wait_thread.start()
              pipe.close()
          
          # Start notebook runner process
          env = os.environ.copy()
          cmd = [
              'python', 'notebook_runner_nbclient.py',
              '-f', 'notebooks/launchable.temp.ipynb',
              '--output-dir', 'notebook_output',
              '--skip-tags', 'ci-skip',
              '-e', f'NGC_API_KEY={env.get("NGC_API_KEY", "")}'
          ]
          
          process = subprocess.Popen(
              cmd,
              stdout=subprocess.PIPE,
              stderr=subprocess.STDOUT,
              text=True,
              bufsize=1,
              env=env
          )
          
          # Save process reference globally for pause/resume mechanism
          notebook_process = process
          
          # Monitor output in separate thread
          # The monitor will pause subprocess when health check is detected
          # Background thread will resume it once HTTP endpoint is ready
          monitor_thread = threading.Thread(target=monitor_output, args=(process.stdout, output_queue), daemon=True)
          monitor_thread.start()
          
          # Wait for process to complete
          return_code = process.wait()
          
          # Wait for monitoring thread to finish
          monitor_thread.join(timeout=1)
          
          sys.exit(return_code)
          EOF
        env:
          # NGC_API_KEY is required for:
          # 1. Authenticating with NGC container registry (nvcr.io) to pull NIM images
          # 2. Accessing NVIDIA hosted NIM endpoints for LLM inference
          # 3. Downloading models from NGC catalog
          # This must be set as a GitHub secret in repository settings:
          # Settings -> Secrets and variables -> Actions -> New repository secret
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
          
          # MODEL_DIRECTORY is set in previous step for model caching
          MODEL_DIRECTORY: ${{ env.MODEL_DIRECTORY }}
          
          # USERID is set in previous step for Docker container user permissions
          USERID: ${{ env.USERID }}

      - name: Cleanup temporary notebook copy
        if: always()
        run: |
          # Remove temporary notebook copy after execution
          rm -f notebooks/launchable.temp.ipynb

      - name: Upload HTML output
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: launchable-notebook-html
          path: |
            notebook_output/*.html
            notebook_output/*.executed.ipynb
          retention-days: 30

      - name: Upload complete output directory
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: notebook-runner-complete-output
          path: notebook_output/
          retention-days: 30
